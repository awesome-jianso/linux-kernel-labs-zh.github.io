<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SO2 Lecture 05 - Symmetric Multi-Processing &mdash; The Linux Kernel  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/asciinema-player.css" type="text/css" />
      <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
      <script>
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/asciinema-player.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="SO2 Lecture 06 - Address Space" href="lec6-address-space.html" />
    <link rel="prev" title="SO2 Lecture 04 - Interrupts" href="lec4-interrupts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            The Linux Kernel
          </a>
              <div class="version">
                5.10.14
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Operating Systems 2</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lec1-intro.html">SO2 Lecture 01 - Course overview and Linux kernel introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec2-syscalls.html">SO2 Lecture 02 - System calls</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec3-processes.html">SO2 Lecture 03 - Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec4-interrupts.html">SO2 Lecture 04 - Interrupts</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">SO2 Lecture 05 - Symmetric Multi-Processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lecture-objectives">Lecture objectives:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#synchronization-basics">Synchronization basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linux-kernel-concurrency-sources">Linux kernel concurrency sources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#atomic-operations">Atomic operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#disabling-preemption-interrupts">Disabling preemption (interrupts)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spin-locks">Spin Locks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cache-coherency-in-multi-processor-systems">Cache coherency in multi-processor systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimized-spin-locks">Optimized spin locks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#process-and-interrupt-context-synchronization">Process and Interrupt Context Synchronization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mutexes">Mutexes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#per-cpu-data">Per CPU data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-ordering-and-barriers">Memory Ordering and Barriers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#read-copy-update-rcu">Read Copy Update (RCU)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lec6-address-space.html">SO2 Lecture 06 - Address Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec7-memory-management.html">SO2 Lecture 07 - Memory Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec8-filesystems.html">SO2 Lecture 08 - Filesystem Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec9-debugging.html">SO2 Lecture 09 - Kernel debugging</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec10-networking.html">SO2 Lecture 10 - Networking</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec11-arch.html">SO2 Lecture 11 - Architecture Layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="lec12-virtualization.html">SO2 Lecture 12 - Virtualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab1-intro.html">SO2 Lab 01 - Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab2-kernel-api.html">SO2 Lab 02 - Kernel API</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab3-device-drivers.html">SO2 Lab 03 - Character device drivers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab4-interrupts.html">SO2 Lab 04 - I/O access and Interrupts</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab5-deferred-work.html">SO2 Lab 05 - Deferred work</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab6-memory-mapping.html">SO2 Lab 06 - Memory Mapping</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab7-block-device-drivers.html">SO2 Lab 07 - Block Device Drivers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab8-filesystems-part1.html">SO2 Lab 08 - File system drivers (Part 1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab9-filesystems-part2.html">SO2 Lab 09 - File system drivers (Part 2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab10-networking.html">SO2 Lab 10 - Networking</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab11-arm-kernel-development.html">SO2 Lab 11 - Kernel Development on ARM</a></li>
<li class="toctree-l2"><a class="reference internal" href="lab12-kernel-profiling.html">SO2 Lab 12 - Kernel Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign-collaboration.html">Collaboration</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign0-kernel-api.html">Assignment 0 - Kernel API</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign1-kprobe-based-tracer.html">Assignment 1 - Kprobe based tracer</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign2-driver-uart.html">Assignment 2 - Driver UART</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign3-software-raid.html">Assignment 3 - Software RAID</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign4-transport-protocol.html">Assignment 4 - SO2 Transport Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="assign7-kvm-vmm.html">Assignment 7 - SO2 Virtual Machine Manager with KVM</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Lectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lectures/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/syscalls.html">系统调用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/processes.html">Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/interrupts.html">Interrupts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/smp.html">Symmetric Multi-Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/address-space.html">Address Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/memory-management.html">Memory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/fs.html">Filesystem Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/networking.html">Network Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/arch.html">Architecture Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lectures/virt.html">Virtualization</a></li>
</ul>
<p class="caption"><span class="caption-text">Labs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../labs/infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/kernel_modules.html">Kernel modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/kernel_api.html">Kernel API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/device_drivers.html">Character device drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/interrupts.html">I/O access and Interrupts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/deferred_work.html">Deferred work</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/block_device_drivers.html">Block Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/filesystems_part1.html">File system drivers (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/filesystems_part2.html">File system drivers (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/networking.html">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/arm_kernel_development.html">Kernel Development on ARM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/memory_mapping.html">Memory mapping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/device_model.html">Linux Device Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs/kernel_profiling.html">Kernel Profiling</a></li>
</ul>
<p class="caption"><span class="caption-text">Useful info</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../info/vm.html">Virtual Machine Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../info/extra-vm.html">Customizing the Virtual Machine Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../info/contributing.html">Contributing to linux-kernel-labs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">The Linux Kernel</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Operating Systems 2</a></li>
      <li class="breadcrumb-item active">SO2 Lecture 05 - Symmetric Multi-Processing</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/so2/lec5-smp.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="so2-lecture-05-symmetric-multi-processing">
<h1>SO2 Lecture 05 - Symmetric Multi-Processing<a class="headerlink" href="#so2-lecture-05-symmetric-multi-processing" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="lec5-smp-slides.html">View slides</a></p>
<span class="admonition-so2-lecture-05-symmetric-multi-processing"></span><div class="section" id="lecture-objectives">
<h2>Lecture objectives:<a class="headerlink" href="#lecture-objectives" title="Permalink to this headline">¶</a></h2>
<ul class="admonition-symmetric-multi-processing simple">
<li>Kernel Concurrency</li>
<li>Atomic operations</li>
<li>Spin locks</li>
<li>Cache thrashing</li>
<li>Optimized spin locks</li>
<li>Process and Interrupt Context Synchronization</li>
<li>Mutexes</li>
<li>Per CPU data</li>
<li>Memory Ordering and Barriers</li>
<li>Read-Copy Update</li>
</ul>
</div>
<div class="section" id="synchronization-basics">
<h2>Synchronization basics<a class="headerlink" href="#synchronization-basics" title="Permalink to this headline">¶</a></h2>
<p>Because the Linux kernel supports symmetric multi-processing (SMP) it
must use a set of synchronization mechanisms to achieve predictable
results, free of race conditions.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We will use the terms core, CPU and processor as
interchangeable for the purpose of this lecture.</p>
</div>
<p>Race conditions can occur when the following two conditions happen
simultaneously:</p>
<ul class="admonition-race-conditions simple">
<li>there are at least two execution contexts that run in &quot;parallel&quot;:<ul>
<li>truly run in parallel (e.g. two system calls running on
different processors)</li>
<li>one of the contexts can arbitrary preempt the other (e.g. an
interrupt preempts a system call)</li>
</ul>
</li>
<li>the execution contexts perform read-write accesses to shared
memory</li>
</ul>
<p>Race conditions can lead to erroneous results that are hard to debug,
because they manifest only when the execution contexts are scheduled
on the CPU cores in a very specific order.</p>
<p>A classical race condition example is an incorrect implementation for
a release operation of a resource counter:</p>
<div class="admonition-race-condition-resource-counter-release highlight-c"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">release_resource</span><span class="p">()</span>
<span class="p">{</span>
     <span class="n">counter</span><span class="o">--</span><span class="p">;</span>

     <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">counter</span><span class="p">)</span>
         <span class="n">free_resource</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A resource counter is used to keep a shared resource available until
the last user releases it but the above implementation has a race
condition that can cause freeing the resource twice:</p>
<p class="admonition-race-condition-scenario">&nbsp;</p>
<img alt="../_images/ditaa-35f7597b35b83bb0025ac2a5f158c9eae23050c8.png" src="../_images/ditaa-35f7597b35b83bb0025ac2a5f158c9eae23050c8.png" />
<p>In most cases the <cite>release_resource()</cite> function will only free the
resource once. However, in the scenario above, if thread A is
preempted right after decrementing <cite>counter</cite> and thread B calls
<cite>release_resource()</cite> it will cause the resource to be freed. When
resumed, thread A will also free the resource since the counter value
is 0.</p>
<p>To avoid race conditions the programmer must first identify the
critical section that can generate a race condition. The critical
section is the part of the code that reads and writes shared memory
from multiple parallel contexts.</p>
<p>In the example above, the minimal critical section is starting with
the counter decrement and ending with checking the counter's value.</p>
<p>Once the critical section has been identified race conditions can be
avoided by using one of the following approaches:</p>
<ul class="admonition-avoiding-race-conditions simple">
<li>make the critical section <strong>atomic</strong> (e.g. use atomic
instructions)</li>
<li><strong>disable preemption</strong> during the critical section (e.g. disable
interrupts, bottom-half handlers, or thread preemption)</li>
<li><strong>serialize the access</strong> to the critical section (e.g. use spin
locks or mutexes to allow only one context or thread in the
critical section)</li>
</ul>
</div>
<div class="section" id="linux-kernel-concurrency-sources">
<h2>Linux kernel concurrency sources<a class="headerlink" href="#linux-kernel-concurrency-sources" title="Permalink to this headline">¶</a></h2>
<p>There are multiple source of concurrency in the Linux kernel that
depend on the kernel configuration as well as the type of system it
runs on:</p>
<ul class="admonition-linux-kernel-concurrency-sources simple">
<li><strong>single core systems</strong>, <strong>non-preemptive kernel</strong>: the current
process can be preempted by interrupts</li>
<li><strong>single core systems</strong>, <strong>preemptive kernel</strong>: above + the
current process can be preempted by other processes</li>
<li><strong>multi-core systems</strong>: above + the current process can run
in parallel with another process or with an interrupt running on
another processor</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We only discuss kernel concurrency and that is why a
non-preemptive kernel running on an single core system
has interrupts as the only source of concurrency.</p>
</div>
</div>
<div class="section" id="atomic-operations">
<h2>Atomic operations<a class="headerlink" href="#atomic-operations" title="Permalink to this headline">¶</a></h2>
<p>In certain circumstances we can avoid race conditions by using atomic
operations that are provided by hardware. Linux provides a unified API
to access atomic operations:</p>
<ul class="admonition-atomic-operations simple">
<li>integer based:<ul>
<li>simple: <code class="xref c c-func docutils literal"><span class="pre">atomic_inc()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">atomic_dec()</span></code>,
<code class="xref c c-func docutils literal"><span class="pre">atomic_add()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">atomic_sub()</span></code></li>
<li>conditional: <code class="xref c c-func docutils literal"><span class="pre">atomic_dec_and_test()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">atomic_sub_and_test()</span></code></li>
</ul>
</li>
<li>bit based:<ul>
<li>simple: <code class="xref c c-func docutils literal"><span class="pre">test_bit()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">set_bit()</span></code>,
<code class="xref c c-func docutils literal"><span class="pre">change_bit()</span></code></li>
<li>conditional: <code class="xref c c-func docutils literal"><span class="pre">test_and_set_bit()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">test_and_clear_bit()</span></code>,
<code class="xref c c-func docutils literal"><span class="pre">test_and_change_bit()</span></code></li>
</ul>
</li>
</ul>
<p>For example, we could use <code class="xref c c-func docutils literal"><span class="pre">atomic_dec_and_test()</span></code> to implement
the resource counter decrement and value checking atomic:</p>
<div class="admonition-using-c-func-atomic-dec-and-test-to-implement-resource-counter-release highlight-c"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">release_resource</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">atomic_dec_and_test</span><span class="p">(</span><span class="o">&amp;</span><span class="n">counter</span><span class="p">))</span>
         <span class="n">free_resource</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>One complication with atomic operations is encountered in
multi-core systems, where an atomic operation is not longer
atomic at the system level (but still atomic at the core level).</p>
<p>To understand why, we need to decompose the atomic operation in memory
loads and stores. Then we can construct race condition scenarios where
the load and store operations are interleaved across CPUs, like in the
example below where incrementing a value from two processors will
produce an unexpected result:</p>
<p class="admonition-atomic-operations-may-not-be-atomic-on-smp-systems">&nbsp;</p>
<img alt="../_images/ditaa-ddd14be50300088958e86912bc5f396797634a3a.png" src="../_images/ditaa-ddd14be50300088958e86912bc5f396797634a3a.png" />
<p>In order to provide atomic operations on SMP systems different
architectures use different techniques. For example, on x86 a LOCK
prefix is used to lock the system bus while executing the prefixed
operation:</p>
<p class="admonition-fixing-atomic-operations-for-smp-systems-x86">&nbsp;</p>
<img alt="../_images/ditaa-c11fccb956cdf115910f9f72e1dc14cd7ed549ff.png" src="../_images/ditaa-c11fccb956cdf115910f9f72e1dc14cd7ed549ff.png" />
<p>On ARM the LDREX and STREX instructions are used together to guarantee
atomic access: LDREX loads a value and signals the exclusive monitor
that an atomic operation is in progress. The STREX attempts to store a
new value but only succeeds if the exclusive monitor has not detected
other exclusive operations. So, to implement atomic operations the
programmer must retry the operation (both LDREX and STREX) until the
exclusive monitor signals a success.</p>
<p>Although they are often interpreted as &quot;light&quot; or &quot;efficient&quot;
synchronization mechanisms (because they &quot;don't require spinning or
context switches&quot;, or because they &quot;are implemented in hardware so
they must be more efficient&quot;, or because they &quot;are just instructions
so they must have similar efficiency as other instructions&quot;), as seen
from the implementation details, atomic operations are actually
expensive.</p>
</div>
<div class="section" id="disabling-preemption-interrupts">
<h2>Disabling preemption (interrupts)<a class="headerlink" href="#disabling-preemption-interrupts" title="Permalink to this headline">¶</a></h2>
<p>On single core systems and non preemptive kernels the only source of
concurrency is the preemption of the current thread by an
interrupt. To prevent concurrency is thus sufficient to disable
interrupts.</p>
<p>This is done with architecture specific instructions, but Linux offers
architecture independent APIs to disable and enable interrupts:</p>
<div class="admonition-synchronization-with-interrupts-x86 highlight-c"><div class="highlight"><pre><span></span> <span class="cp">#define local_irq_disable() \</span>
<span class="cp">     asm volatile („cli” : : : „memory”)</span>

<span class="cp">#define local_irq_enable() \</span>
<span class="cp">    asm volatile („sti” : : : „memory”)</span>

<span class="cp">#define local_irq_save(flags) \</span>
<span class="cp">    asm volatile (&quot;pushf ; pop %0&quot; :&quot;=g&quot; (flags)</span>
                  <span class="o">:</span> <span class="cm">/* no input */</span><span class="o">:</span> <span class="s">&quot;memory&quot;</span><span class="p">)</span> \
    <span class="k">asm</span> <span class="k">volatile</span><span class="p">(</span><span class="s">&quot;cli&quot;</span><span class="o">:</span> <span class="o">:</span> <span class="o">:</span><span class="s">&quot;memory&quot;</span><span class="p">)</span>

<span class="cp">#define local_irq_restore(flags) \</span>
<span class="cp">    asm volatile (&quot;push %0 ; popf&quot;</span>
                  <span class="o">:</span> <span class="cm">/* no output */</span>
                  <span class="o">:</span> <span class="s">&quot;g&quot;</span> <span class="p">(</span><span class="n">flags</span><span class="p">)</span> <span class="o">:</span><span class="s">&quot;memory&quot;</span><span class="p">,</span> <span class="s">&quot;cc&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>Although the interrupts can be explicitly disabled and enable with
<code class="xref c c-func docutils literal"><span class="pre">local_irq_disable()</span></code> and <code class="xref c c-func docutils literal"><span class="pre">local_irq_enable()</span></code> these APIs
should only be used when the current state and interrupts is
known. They are usually used in core kernel code (like interrupt
handling).</p>
<p>For typical cases where we want to avoid interrupts due to concurrency
issues it is recommended to use the <code class="xref c c-func docutils literal"><span class="pre">local_irq_save()</span></code> and
<code class="xref c c-func docutils literal"><span class="pre">local_irq_restore()</span></code> variants. They take care of saving and
restoring the interrupts states so they can be freely called from
overlapping critical sections without the risk of accidentally
enabling interrupts while still in a critical section, as long as the
calls are balanced.</p>
</div>
<div class="section" id="spin-locks">
<h2>Spin Locks<a class="headerlink" href="#spin-locks" title="Permalink to this headline">¶</a></h2>
<p>Spin locks are used to serialize access to a critical section. They
are necessary on multi-core systems where we can have true execution
parallelism. This is a typical spin lock implementation:</p>
<div class="admonition-spin-lock-implementation-example-x86 highlight-asm"><div class="highlight"><pre><span></span>spin_lock:
    lock bts [my_lock], 0
    jc spin_lock

/* critical section */

spin_unlock:
    mov [my_lock], 0
</pre></div>
</div>
<p><strong>bts dts, src</strong> - bit test and set; it copies the src bit from the dts
memory address to the carry flag and then sets it:</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="n">CF</span> <span class="o">&lt;-</span> <span class="n">dts</span><span class="p">[</span><span class="n">src</span><span class="p">]</span>
<span class="n">dts</span><span class="p">[</span><span class="n">src</span><span class="p">]</span> <span class="o">&lt;-</span> <span class="mi">1</span>
</pre></div>
</div>
<p>As it can be seen, the spin lock uses an atomic instruction to make
sure that only one core can enter the critical section. If there are
multiple cores trying to enter they will continuously &quot;spin&quot; until the
lock is released.</p>
<p>While the spin lock avoids race conditions, it can have a significant
impact on the system's performance due to &quot;lock contention&quot;:</p>
<ul class="admonition-lock-contention simple">
<li>There is lock contention when at least one core spins trying to
enter the critical section lock</li>
<li>Lock contention grows with the critical section size, time spent
in the critical section and the number of cores in the system</li>
</ul>
<p>Another negative side effect of spin locks is cache thrashing.</p>
<p class="admonition-cache-thrashing">Cache thrashing occurs when multiple cores are trying to read and
write to the same memory resulting in excessive cache misses.</p>
<p>Since spin locks continuously access memory during lock contention,
cache thrashing is a common occurrence due to the way cache
coherency is implemented.</p>
</div>
<div class="section" id="cache-coherency-in-multi-processor-systems">
<h2>Cache coherency in multi-processor systems<a class="headerlink" href="#cache-coherency-in-multi-processor-systems" title="Permalink to this headline">¶</a></h2>
<p>The memory hierarchy in multi-processor systems is composed of local
CPU caches (L1 caches), shared CPU caches (L2 caches) and the main
memory. To explain cache coherency we will ignore the L2 cache and
only consider the L1 caches and main memory.</p>
<p>In the figure below we present a view of the memory hierarchy with two
variables A and B that fall into different cache lines and where
caches and the main memory are synchronized:</p>
<p class="admonition-synchronized-caches-and-memory">&nbsp;</p>
<img alt="../_images/ditaa-4d63c157487ff8291f2a6e93fe680ec38c1a3212.png" src="../_images/ditaa-4d63c157487ff8291f2a6e93fe680ec38c1a3212.png" />
<p>In the absence of a synchronization mechanism between the caches and
main memory, when CPU 0 executes <cite>A = A + B</cite> and CPU 1 executes <cite>B =
A + B</cite> we will have the following memory view:</p>
<p class="admonition-unsynchronized-caches-and-memory">&nbsp;</p>
<img alt="../_images/ditaa-7ee0f9bb5f5af586e043afd47cfbad0adcc34888.png" src="../_images/ditaa-7ee0f9bb5f5af586e043afd47cfbad0adcc34888.png" />
<p>In order to avoid the situation above multi-processor systems use
cache coherency protocols. There are two main types of cache coherency
protocols:</p>
<ul class="admonition-cache-coherency-protocols simple">
<li>Bus snooping (sniffing) based: memory bus transactions are
monitored by caches and they take actions to preserve
coherency</li>
<li>Directory based: there is a separate entity (directory) that
maintains the state of caches; caches interact with directory
to preserve coherency</li>
</ul>
<p>Bus snooping is simpler but it performs poorly when the number of
cores goes beyond 32-64.</p>
<p>Directory based cache coherence protocols scale much better (up
to thousands of cores) and are usually used in NUMA systems.</p>
<p>A simple cache coherency protocol that is commonly used in practice is
MESI (named after the acronym of the cache line states names:
<strong>Modified</strong>, <strong>Exclusive</strong>, <strong>Shared</strong> and <strong>Invalid</strong>). It's main
characteristics are:</p>
<ul class="admonition-mesi-cache-coherence-protocol simple">
<li>Caching policy: write back</li>
<li>Cache line states<ul>
<li>Modified: owned by a single core and dirty</li>
<li>Exclusive: owned by a single core and clean</li>
<li>Shared: shared between multiple cores and clean</li>
<li>Invalid : the line is not cached</li>
</ul>
</li>
</ul>
<p>Issuing read or write requests from CPU cores will trigger state
transitions, as exemplified below:</p>
<ul class="admonition-mesi-state-transitions simple">
<li>Invalid -&gt; Exclusive: read request, all other cores have the line
in Invalid; line loaded from memory</li>
<li>Invalid -&gt; Shared: read request, at least one core has the line
in Shared or Exclusive; line loaded from sibling cache</li>
<li>Invalid/Shared/Exclusive -&gt; Modified: write request; <strong>all
other</strong> cores <strong>invalidate</strong> the line</li>
<li>Modified -&gt; Invalid: write request from other core; line is
flushed to memory</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The most important characteristic of the MESI protocol is
that it is a write-invalidate cache protocol. When writing to a
shared location all other caches are invalidated.</p>
</div>
<p>This has important performance impact in certain access patterns, and
one such pattern is contention for a simple spin lock implementation
like we discussed above.</p>
<p>To exemplify this issue lets consider a system with three CPU cores,
where the first has acquired the spin lock and it is running the
critical section while the other two are spinning waiting to enter the
critical section:</p>
<p class="admonition-cache-thrashing-due-to-spin-lock-contention">&nbsp;</p>
<img alt="../_images/ditaa-b26d802c286bda6c559b4dcfa8a7fb27f840463e.png" src="../_images/ditaa-b26d802c286bda6c559b4dcfa8a7fb27f840463e.png" />
<p>As it can be seen from the figure above due to the writes issued by
the cores spinning on the lock we see frequent cache line invalidate
operations which means that basically the two waiting cores will flush
and load the cache line while waiting for the lock, creating
unnecessary traffic on the memory bus and slowing down memory accesses
for the first core.</p>
<p>Another issue is that most likely data accessed by the first CPU
during the critical section is stored in the same cache line with the
lock (common optimization to have the data ready in the cache after
the lock is acquired). Which means that the cache invalidation
triggered by the two other spinning cores will slow down the execution
of the critical section which in turn triggers more cache invalidate
actions.</p>
</div>
<div class="section" id="optimized-spin-locks">
<h2>Optimized spin locks<a class="headerlink" href="#optimized-spin-locks" title="Permalink to this headline">¶</a></h2>
<p>As we have seen simple spin lock implementations can have poor
performance issues due to cache thrashing, especially as the number of
cores increase. To avoid this issue there are two possible strategies:</p>
<ul class="simple">
<li>reduce the number of writes and thus reduce the number of cache
invalidate operations</li>
<li>avoid the other processors spinning on the same cache line, and thus
avoid the cache invalidate operations</li>
</ul>
<p>An optimized spin lock implementation that uses the first approach is
presented below:</p>
<p class="admonition-optimized-spin-lock-keacquirespinlock">&nbsp;</p>
<div class="highlight-asm"><div class="highlight"><pre><span></span><span class="nl">spin_lock:</span>
    <span class="na">rep</span> <span class="c">; nop</span>
    <span class="nf">test</span> <span class="no">lock_addr</span><span class="p">,</span> <span class="mi">1</span>
    <span class="nf">jnz</span> <span class="no">spin_lock</span>
    <span class="na">lock</span> <span class="nf">bts</span> <span class="no">lock_addr</span>
    <span class="nf">jc</span> <span class="no">spin_lock</span>
</pre></div>
</div>
<ul class="simple">
<li>we first test the lock read only, using a non atomic
instructions, to avoid writes and thus invalidate operations
while we spin</li>
<li>only when the lock <em>might</em> be free, we try to acquire it</li>
</ul>
<p>The implementation also use the <strong>PAUSE</strong> instruction to avoid
pipeline flushes due to (false positive) memory order violations and
to add a small delay (proportional with the memory bus frequency) to
reduce power consumption.</p>
<p>A similar implementation with support for fairness (the CPU cores are
allowed in the critical section based on the time of arrival) is used
in the Linux kernel (the <a class="reference external" href="https://lwn.net/Articles/267968/">ticket spin lock</a>)
for many architectures.</p>
<p>However, for the x86 architecture, the current spin lock
implementation uses a queued spin lock where the CPU cores spin on
different locks (hopefully distributed in different cache lines) to
avoid cache invalidation operations:</p>
<p class="admonition-queued-spin-locks">&nbsp;</p>
<img alt="../_images/ditaa-58545831034f050660727be99cede213bc4a53c7.png" src="../_images/ditaa-58545831034f050660727be99cede213bc4a53c7.png" />
<p>Conceptually, when a new CPU core tries to acquire the lock and it
fails it will add its private lock to the list of waiting CPU
cores. When the lock owner exits the critical section it unlocks the
next lock in the list, if any.</p>
<p>While a read spin optimized spin lock reduces most of the cache
invalidation operations, the lock owner can still generate cache
invalidate operations due to writes to data structures close to the
lock and thus part of the same cache line. This in turn generates
memory traffic on subsequent reads on the spinning cores.</p>
<p>Hence, queued spin locks scale much better for large number of cores
as is the case for NUMA systems. And since they have similar fairness
properties as the ticket lock it is the preferred implementation on
the x86 architecture.</p>
</div>
<div class="section" id="process-and-interrupt-context-synchronization">
<h2>Process and Interrupt Context Synchronization<a class="headerlink" href="#process-and-interrupt-context-synchronization" title="Permalink to this headline">¶</a></h2>
<p>Accessing shared data from both process and interrupt context is a
relatively common scenario. On single core systems we can do this by
disabling interrupts, but that won't work on multi-core systems,
as we can have the process running on one CPU core and the interrupt
context running on a different CPU core.</p>
<p>Using a spin lock, which was designed for multi-processor systems,
seems like the right solution, but doing so can cause common
deadlock conditions, as detailed by the following scenario:</p>
<ul class="admonition-process-and-interrupt-handler-synchronization-deadlock simple">
<li>In the process context we take the spin lock</li>
<li>An interrupt occurs and it is scheduled on the same CPU core</li>
<li>The interrupt handler runs and tries to take the spin lock</li>
<li>The current CPU will deadlock</li>
</ul>
<p>To avoid this issue a two fold approach is used:</p>
<ul class="admonition-interrupt-synchronization-for-smp simple">
<li>In process context: disable interrupts and acquire a spin lock;
this will protect both against interrupt or other CPU cores race
conditions (<code class="xref c c-func docutils literal"><span class="pre">spin_lock_irqsave()</span></code> and
<code class="xref c c-func docutils literal"><span class="pre">spin_lock_restore()</span></code> combine the two operations)</li>
<li>In interrupt context: take a spin lock; this will will protect
against race conditions with other interrupt handlers or process
context running on different processors</li>
</ul>
<p>We have the same issue for other interrupt context handlers such as
softirqs, tasklets or timers and while disabling interrupts might
work, it is recommended to use dedicated APIs:</p>
<ul class="admonition-bottom-half-synchronization-for-smp simple">
<li>In process context use <code class="xref c c-func docutils literal"><span class="pre">spin_lock_bh()</span></code> (which combines
<code class="xref c c-func docutils literal"><span class="pre">local_bh_disable()</span></code> and <code class="xref c c-func docutils literal"><span class="pre">spin_lock()</span></code>) and
<code class="xref c c-func docutils literal"><span class="pre">spin_unlock_bh()</span></code> (which combines <code class="xref c c-func docutils literal"><span class="pre">spin_unlock()</span></code> and
<code class="xref c c-func docutils literal"><span class="pre">local_bh_enable()</span></code>)</li>
<li>In bottom half context use: <code class="xref c c-func docutils literal"><span class="pre">spin_lock()</span></code> and
<code class="xref c c-func docutils literal"><span class="pre">spin_unlock()</span></code> (or <code class="xref c c-func docutils literal"><span class="pre">spin_lock_irqsave()</span></code> and
<code class="xref c c-func docutils literal"><span class="pre">spin_lock_irqrestore()</span></code> if sharing data with interrupt
handlers)</li>
</ul>
<p>As mentioned before, another source of concurrency in the Linux kernel
can be other processes, due to preemption.</p>
<p class="admonition-preemption">&nbsp;</p>
<p>Preemption is configurable: when active it provides better latency
and response time, while when deactivated it provides better
throughput.</p>
<p>Preemption is disabled by spin locks and mutexes but it can be
manually disabled as well (by core kernel code).</p>
<p>As for local interrupt enabling and disabling APIs, the bottom half
and preemption APIs allows them to be used in overlapping critical
sections. A counter is used to track the state of bottom half and
preemption. In fact the same counter is used, with different increment
values:</p>
<div class="admonition-preemption-and-bottom-half-masking highlight-c"><div class="highlight"><pre><span></span><span class="cp">#define PREEMPT_BITS      8</span>
<span class="cp">#define SOFTIRQ_BITS      8</span>
<span class="cp">#define HARDIRQ_BITS      4</span>
<span class="cp">#define NMI_BITS          1</span>

<span class="cp">#define preempt_disable() preempt_count_inc()</span>

<span class="cp">#define local_bh_disable() add_preempt_count(SOFTIRQ_OFFSET)</span>

<span class="cp">#define local_bh_enable() sub_preempt_count(SOFTIRQ_OFFSET)</span>

<span class="cp">#define irq_count() (preempt_count() &amp; (HARDIRQ_MASK | SOFTIRQ_MASK))</span>

<span class="cp">#define in_interrupt() irq_count()</span>

<span class="n">asmlinkage</span> <span class="kt">void</span> <span class="nf">do_softirq</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">in_interrupt</span><span class="p">())</span> <span class="k">return</span><span class="p">;</span>
    <span class="p">...</span>
</pre></div>
</div>
</div>
<div class="section" id="mutexes">
<h2>Mutexes<a class="headerlink" href="#mutexes" title="Permalink to this headline">¶</a></h2>
<p>Mutexes are used to protect against race conditions from other CPU
cores but they can only be used in <strong>process context</strong>. As opposed to
spin locks, while a thread is waiting to enter the critical section it
will not use CPU time, but instead it will be added to a waiting queue
until the critical section is vacated.</p>
<p>Since mutexes and spin locks usage intersect, it is useful to compare
the two:</p>
<ul class="admonition-mutexes simple">
<li>They don't &quot;waste&quot; CPU cycles; system throughput is better than
spin locks if context switch overhead is lower than medium
spinning time</li>
<li>They can't be used in interrupt context</li>
<li>They have a higher latency than spin locks</li>
</ul>
<p>Conceptually, the <code class="xref c c-func docutils literal"><span class="pre">mutex_lock()</span></code> operation is relatively simple:
if the mutex is not acquired we can take the fast path via an atomic
exchange operation:</p>
<div class="admonition-c-func-mutex-lock-fast-path highlight-c"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="n">__sched</span> <span class="nf">mutex_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">mutex</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">might_sleep</span><span class="p">();</span>

  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">__mutex_trylock_fast</span><span class="p">(</span><span class="n">lock</span><span class="p">))</span>
    <span class="n">__mutex_lock_slowpath</span><span class="p">(</span><span class="n">lock</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="n">__always_inline</span> <span class="kt">bool</span> <span class="nf">__mutex_trylock_fast</span><span class="p">(</span><span class="k">struct</span> <span class="n">mutex</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">curr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">current</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">atomic_long_cmpxchg_acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">owner</span><span class="p">,</span> <span class="mi">0UL</span><span class="p">,</span> <span class="n">curr</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>

  <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>otherwise we take the slow path where we add ourselves to the mutex
waiting list and put ourselves to sleep:</p>
<div class="admonition-c-func-mutex-lock-slow-path highlight-c"><div class="highlight"><pre><span></span><span class="p">...</span>
  <span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
<span class="p">...</span>
  <span class="cm">/* add waiting tasks to the end of the waitqueue (FIFO): */</span>
  <span class="n">list_add_tail</span><span class="p">(</span><span class="o">&amp;</span><span class="n">waiter</span><span class="p">.</span><span class="n">list</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_list</span><span class="p">);</span>
<span class="p">...</span>
  <span class="n">waiter</span><span class="p">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">current</span><span class="p">;</span>
<span class="p">...</span>
  <span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">__mutex_trylock</span><span class="p">(</span><span class="n">lock</span><span class="p">))</span>
      <span class="k">goto</span> <span class="n">acquired</span><span class="p">;</span>
  <span class="p">...</span>
    <span class="n">spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
  <span class="p">...</span>
    <span class="n">set_current_state</span><span class="p">(</span><span class="n">state</span><span class="p">);</span>
    <span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
<span class="nl">acquired</span><span class="p">:</span>
  <span class="n">__set_current_state</span><span class="p">(</span><span class="n">TASK_RUNNING</span><span class="p">);</span>
  <span class="n">mutex_remove_waiter</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">waiter</span><span class="p">,</span> <span class="n">current</span><span class="p">);</span>
  <span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
<span class="p">...</span>
</pre></div>
</div>
<p>The full implementation is a bit more complex: instead of going to
sleep immediately it optimistic spinning if it detects that the lock
owner is currently running on a different CPU as chances are the owner
will release the lock soon. It also checks for signals and handles
mutex debugging for locking dependency engine debug feature.</p>
<p>The <code class="xref c c-func docutils literal"><span class="pre">mutex_unlock()</span></code> operation is symmetric: if there are no
waiters on the mutex then we can take the fast path via an atomic exchange
operation:</p>
<div class="admonition-c-func-mutex-unlock-fast-path highlight-c"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="n">__sched</span> <span class="nf">mutex_unlock</span><span class="p">(</span><span class="k">struct</span> <span class="n">mutex</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">__mutex_unlock_fast</span><span class="p">(</span><span class="n">lock</span><span class="p">))</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="n">__mutex_unlock_slowpath</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="n">_RET_IP_</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">static</span> <span class="n">__always_inline</span> <span class="kt">bool</span> <span class="nf">__mutex_unlock_fast</span><span class="p">(</span><span class="k">struct</span> <span class="n">mutex</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">curr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span><span class="n">current</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">atomic_long_cmpxchg_release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">owner</span><span class="p">,</span> <span class="n">curr</span><span class="p">,</span> <span class="mi">0UL</span><span class="p">)</span> <span class="o">==</span> <span class="n">curr</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>

  <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">__mutex_lock_slowpath</span><span class="p">(</span><span class="k">struct</span> <span class="n">mutex</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
<span class="p">{</span>
<span class="p">...</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">__mutex_waiter_is_first</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">waiter</span><span class="p">))</span>
          <span class="n">__mutex_set_flag</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="n">MUTEX_FLAG_WAITERS</span><span class="p">);</span>
<span class="p">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Because <code class="xref c c-type docutils literal"><span class="pre">struct</span> <span class="pre">task_struct</span></code> is cached aligned the 7
lower bits of the owner field can be used for various flags,
such as <code class="xref c c-type docutils literal"><span class="pre">MUTEX_FLAG_WAITERS</span></code>.</p>
</div>
<p>Otherwise we take the slow path where we pick up first waiter from the
list and wake it up:</p>
<div class="admonition-c-func-mutex-unlock-slow-path highlight-c"><div class="highlight"><pre><span></span><span class="p">...</span>
<span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_list</span><span class="p">))</span> <span class="p">{</span>
  <span class="cm">/* get the first entry from the wait-list: */</span>
  <span class="k">struct</span> <span class="n">mutex_waiter</span> <span class="o">*</span><span class="n">waiter</span><span class="p">;</span>
  <span class="n">waiter</span> <span class="o">=</span> <span class="n">list_first_entry</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_list</span><span class="p">,</span> <span class="k">struct</span> <span class="n">mutex_waiter</span><span class="p">,</span>
                            <span class="n">list</span><span class="p">);</span>
  <span class="n">next</span> <span class="o">=</span> <span class="n">waiter</span><span class="o">-&gt;</span><span class="n">task</span><span class="p">;</span>
  <span class="n">wake_q_add</span><span class="p">(</span><span class="o">&amp;</span><span class="n">wake_q</span><span class="p">,</span> <span class="n">next</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">...</span>
<span class="n">spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">wait_lock</span><span class="p">);</span>
<span class="p">...</span>
<span class="n">wake_up_q</span><span class="p">(</span><span class="o">&amp;</span><span class="n">wake_q</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="per-cpu-data">
<h2>Per CPU data<a class="headerlink" href="#per-cpu-data" title="Permalink to this headline">¶</a></h2>
<p>Per CPU data avoids race conditions by avoiding to use shared
data. Instead, an array sized to the maximum possible CPU cores is
used and each core will use its own array entry to read and write
data. This approach certainly has advantages:</p>
<ul class="admonition-per-cpu-data simple">
<li>No need to synchronize to access the data</li>
<li>No contention, no performance impact</li>
<li>Well suited for distributed processing where aggregation is only
seldom necessary (e.g. statistics counters)</li>
</ul>
</div>
<div class="section" id="memory-ordering-and-barriers">
<h2>Memory Ordering and Barriers<a class="headerlink" href="#memory-ordering-and-barriers" title="Permalink to this headline">¶</a></h2>
<p>Modern processors and compilers employ out-of-order execution to
improve performance. For example, processors can execute &quot;future&quot;
instructions while waiting for current instruction data to be fetched
from memory.</p>
<p>Here is an example of out of order compiler generated code:</p>
<table border="1" class="admonition-out-of-order-compiler-generated-code docutils">
<colgroup>
<col width="43%" />
<col width="57%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>C code</td>
<td>Compiler generated code</td>
</tr>
<tr class="row-even"><td><div class="first last highlight-c"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
</pre></div>
</div>
</td>
<td><div class="first last highlight-asm"><div class="highlight"><pre><span></span><span class="nf">MOV</span> <span class="no">R10</span><span class="p">,</span> <span class="mi">1</span>
<span class="nf">MOV</span> <span class="no">R11</span><span class="p">,</span> <span class="mi">2</span>
<span class="nf">STORE</span> <span class="no">R11</span><span class="p">,</span> <span class="no">b</span>
<span class="nf">STORE</span> <span class="no">R10</span><span class="p">,</span> <span class="no">a</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When executing instructions out of order the processor makes
sure that data dependency is observed, i.e. it won't execute
instructions whose input depend on the output of a previous
instruction that has not been executed.</p>
</div>
<p>In most cases out of order execution is not an issue. However, in
certain situations (e.g. communicating via shared memory between
processors or between processors and hardware) we must issue some
instructions before others even without data dependency between them.</p>
<p>For this purpose we can use barriers to order memory operations:</p>
<ul class="admonition-barriers simple">
<li>A read barrier (<code class="xref c c-func docutils literal"><span class="pre">rmb()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">smp_rmb()</span></code>) is used to
make sure that no read operation crosses the barrier; that is,
all read operation before the barrier are complete before
executing the first instruction after the barrier</li>
<li>A write barrier (<code class="xref c c-func docutils literal"><span class="pre">wmb()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">smp_wmb()</span></code>) is used to
make sure that no write operation crosses the barrier</li>
<li>A simple barrier (<code class="xref c c-func docutils literal"><span class="pre">mb()</span></code>, <code class="xref c c-func docutils literal"><span class="pre">smp_mb()</span></code>) is used
to make sure that no write or read operation crosses the barrier</li>
</ul>
</div>
<div class="section" id="read-copy-update-rcu">
<h2>Read Copy Update (RCU)<a class="headerlink" href="#read-copy-update-rcu" title="Permalink to this headline">¶</a></h2>
<p>Read Copy Update is a special synchronization mechanism similar with
read-write locks but with significant improvements over it (and some
limitations):</p>
<ul class="admonition-read-copy-update-rcu simple">
<li><strong>Read-only</strong> lock-less access at the same time with write access</li>
<li>Write accesses still requires locks in order to avoid races
between writers</li>
<li>Requires unidirectional traversal by readers</li>
</ul>
<p>In fact, the read-write locks in the Linux kernel have been deprecated
and then removed, in favor of RCU.</p>
<p>Implementing RCU for a new data structure is difficult, but a few
common data structures (lists, queues, trees) do have RCU APIs that
can be used.</p>
<p>RCU splits removal updates to the data structures in two phases:</p>
<ul class="admonition-removal-and-reclamation simple">
<li><strong>Removal</strong>: removes references to elements. Some old readers may
still see the old reference so we can't free the element.</li>
<li><strong>Elimination</strong>: free the element. This action is postponed until
all existing readers finish traversal (quiescent cycle). New
readers won't affect the quiescent cycle.</li>
</ul>
<p>As an example, lets take a look on how to delete an element from a
list using RCU:</p>
<p class="admonition-rcu-list-delete">&nbsp;</p>
<img alt="../_images/ditaa-5193a924360bebc83d2f81188cd0b0093ec01e6a.png" src="../_images/ditaa-5193a924360bebc83d2f81188cd0b0093ec01e6a.png" />
<p>In the first step it can be seen that while readers traverse the list
all elements are referenced. In step two a writer removes
element B. Reclamation is postponed since there are still readers that
hold references to it. In step three a quiescent cycle just expired
and it can be noticed that there are no more references to
element B. Other elements still have references from readers that
started the list traversal after the element was removed. In step 4 we
finally perform reclamation (free the element).</p>
<p>Now that we covered how RCU functions at the high level, lets looks at
the APIs for traversing the list as well as adding and removing an
element to the list:</p>
<div class="admonition-rcu-list-apis-cheat-sheet highlight-c"><div class="highlight"><pre><span></span><span class="cm">/* list traversal */</span>
<span class="n">rcu_read_lock</span><span class="p">();</span>
<span class="n">list_for_each_entry_rcu</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">head</span><span class="p">)</span> <span class="p">{</span>
  <span class="cm">/* no sleeping, blocking calls or context switch allowed */</span>
<span class="p">}</span>
<span class="n">rcu_read_unlock</span><span class="p">();</span>


<span class="cm">/* list element delete  */</span>
<span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span>
<span class="n">list_del_rcu</span><span class="p">(</span><span class="o">&amp;</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">list</span><span class="p">);</span>
<span class="n">spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span>
<span class="n">synchronize_rcu</span><span class="p">();</span>
<span class="n">kfree</span><span class="p">(</span><span class="n">node</span><span class="p">);</span>

<span class="cm">/* list element add  */</span>
<span class="n">spin_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span>
<span class="n">list_add_rcu</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">list</span><span class="p">);</span>
<span class="n">spin_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lec4-interrupts.html" class="btn btn-neutral float-left" title="SO2 Lecture 04 - Interrupts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lec6-address-space.html" class="btn btn-neutral float-right" title="SO2 Lecture 06 - Address Space" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright The kernel development community.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>